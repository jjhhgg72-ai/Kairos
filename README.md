---

# SAFRS Camera Raspberry Pi

## AI Vision & Target Tracking Node (UDP Cluster Version)

The **Camera Raspberry Pi** is responsible for **visual perception, object detection, and target tracking** within the SAFRS AGV UDP-based distributed robotics system.

This node directly interfaces with a **USB Camera**, utilizes **TensorFlow Lite (TFLite)** for efficient edge inference, and calculates **targeting error vectors** to guide the turret or robot chassis, while publishing detection status and end signals back to the cluster.

---

## ğŸ§  Role in SAFRS System

Camera Raspberry Pi acts as the **"Eyes" of the system**.

### Responsibilities

* Capture video frames from **USB Camera** (640x480 @ 30fps)
* Perform **Object Detection** (EfficientDet-Lite1) to find potential targets
* Perform **Classification** (MobileNet + Color Heuristics) to distinguish **Ally vs. Enemy**
* Calculate **visual error (X/Y offset)** from the image center
* **Publish Data:**

| Topic Name | Description |
| --- | --- |
| `/error_xy` | Targeting correction vector |
| `/detect` | Target information |
| `/end` | Task completion signal |

* Manage **Camera State (ON/OFF)** based on System Mode

---

## ğŸ“¡ Communication Overview

### Subscribed Topics

* `/mode` (`system_interfaces/msg/SystemMode`)
* Controls FSM (BOOT, STANDBY, TRACK_ALLY, TRACK_ENEMY, NAVI)



### Published Topics

* `/error_xy` (`geometry_msgs/msg/Vector3`)
* `x`: Horizontal error (pixels from center)
* `y`: Vertical error (pixels from center)
* `z`: Status flag (0.0 = Tracking, -1.0 = Lost)


* `/detect` (`system_interfaces/msg/Detect`)
* Label, Center X, Center Y


* `/end` (`system_interfaces/msg/EndSignal`)
* Triggered when target is centered and confidence is high



---

## ğŸ“ Directory Structure

```text
camera_client_cluster2/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ EfficientDet-Lite1.tflite           # Object Detection Model
â”‚   â””â”€â”€ monkey_classifier_quant_int8.tflite # Classification Model
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ camera_client_node3.py              # Main Application (Provided Code)
â”‚
â”œâ”€â”€ package.xml
â””â”€â”€ setup.py

```

---

## âš™ï¸ Configuration & Constants

### 1ï¸âƒ£ Model Configuration

Hardcoded paths in `CameraNode`:

* **Detection:** `EfficientDet-Lite1.tflite` (Input: Dynamic/384x384)
* **Classification:** `monkey_classifier_quant_int8.tflite` (Input: 224x224)

### 2ï¸âƒ£ Camera Settings

* Resolution: **640x480**
* Exposure: **-5** (Darkened for better color segmentation)
* Auto Exposure: **Enabled**

### 3ï¸âƒ£ Logic Thresholds

* `CENTER_THRESHOLD`: **25 pixels** (Tolerance for "aimed" state)
* `CAMERA_OFF_DELAY`: **5.0 seconds** (Cool-down before releasing resource)

---

## ğŸš€ How to Run

### Launch Camera Node

```bash
ros2 run camera_client_cluster2 camera_client_node3

```

If the camera is connected correctly, you should see:

* `[INFO] CameraNode started...`
* `[INFO] Camera ON` (When mode switches to STANDBY/TRACK)
* A GUI window "Camera View" showing the feed with bounding boxes.

---

## ğŸ§© Internal Control Flow

```text
Camera Frame (640x480)
    â†“
Resize (to Model Input, e.g., 384x384)
    â†“
TFLite Inference (EfficientDet)
    â†“
Bounding Box Extraction
    â†“
ROI Crop & Color Analysis (Red/Blue Ratio)
    â†“
Secondary Inference (MobileNet Classifier)
    â†“
Logic: Determine ALLY / ENEMY
    â†“
Calculate Error (Center - Object)
    â†“
Publish /error_xy & /detect

```

---

## âš ï¸ Notes & Best Practices

* **Lighting matters:** The camera exposure is set low (-5) to highlight colored LEDs/markers. Ensure consistent lighting.
* **Thread Safety:** Image capture and Inference run in separate threads (`capture_loop`, `inference_loop`) using `threading.Lock()` to prevent race conditions.
* **Model Paths:** Ensure absolute paths to `.tflite` files are correct in the script.
* **Performance:**
* Raspberry Pi 4: Expect ~5-10 FPS inference.
* PC/Jetson: Expect 30+ FPS.



---

## ğŸ”§ Hardware Assumptions

* **Camera:** Standard USB Webcam (Video0)
* **Compute:** Raspberry Pi 4 or equivalent (ARM64)
* **Dependencies:**
* `tflite_runtime` or `tensorflow`
* `opencv-python`
* `ros2-humble` (or compatible)



---

## ğŸ“œ License

SAFRS Robotics Platform

License: MIT

---

## ğŸ™‹ Maintainer

**ì§€ìœ¤ëª©ì¥**

SAFRS Robotics Team

---
